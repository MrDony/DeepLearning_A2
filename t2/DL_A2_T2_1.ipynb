{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfPUh17t_aQ5",
        "outputId": "0942725f-5679-4c2d-e936-53c4bc4651c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_FOLDER = 'drive/MyDrive/A2_Data/T3_Data/images/'\n",
        "_TRAIN_ANGRY_FOLDER = _FOLDER + 'train/angry'\n",
        "_TRAIN_DISGUST_FOLDER = _FOLDER + 'train/disgust'\n",
        "_TRAIN_FEAR_FOLDER = _FOLDER + 'train/fear'\n",
        "_TRAIN_HAPPY_FOLDER = _FOLDER + 'train/happy'\n",
        "_TRAIN_NEUTRAL_FOLDER = _FOLDER + 'train/neutral'\n",
        "_TRAIN_SAD_FOLDER = _FOLDER + 'train/sad'\n",
        "_TRAIN_SURPRISE_FOLDER = _FOLDER + 'train/surprise'\n",
        "\n",
        "_VALIDATION_ANGRY_FOLDER = _FOLDER + 'validation/angry'\n",
        "_VALIDATION_DISGUST_FOLDER = _FOLDER + 'validation/disgust'\n",
        "_VALIDATION_FEAR_FOLDER = _FOLDER + 'validation/fear'\n",
        "_VALIDATION_HAPPY_FOLDER = _FOLDER + 'validation/happy'\n",
        "_VALIDATION_NEUTRAL_FOLDER = _FOLDER + 'validation/neutral'\n",
        "_VALIDATION_SAD_FOLDER = _FOLDER + 'validation/sad'\n",
        "_VALIDATION_SURPRISE_FOLDER = _FOLDER + 'validation/surprise'\n",
        "\n",
        "_TRAINING_DIRECTORIES_LABELS = [\n",
        "  (_TRAIN_ANGRY_FOLDER,'angry'),\n",
        "  (_TRAIN_DISGUST_FOLDER,'disgust'),\n",
        "  (_TRAIN_FEAR_FOLDER,'fear'),\n",
        "  (_TRAIN_HAPPY_FOLDER,'happy'),\n",
        "  (_TRAIN_NEUTRAL_FOLDER,'neutral'),\n",
        "  (_TRAIN_SAD_FOLDER,'sad'),\n",
        "  (_TRAIN_SURPRISE_FOLDER,'surprise')\n",
        "]\n",
        "\n",
        "_VALIDATION_DIRECTORIES_LABELS = [\n",
        "  (_VALIDATION_ANGRY_FOLDER,'angry'),\n",
        "  (_VALIDATION_DISGUST_FOLDER,'disgust'),\n",
        "  (_VALIDATION_FEAR_FOLDER,'fear'),\n",
        "  (_VALIDATION_HAPPY_FOLDER,'happy'),\n",
        "  (_VALIDATION_NEUTRAL_FOLDER,'neutral'),\n",
        "  (_VALIDATION_SAD_FOLDER,'sad'),\n",
        "  (_VALIDATION_SURPRISE_FOLDER,'surprise')\n",
        "]"
      ],
      "metadata": {
        "id": "Kt_W67rMfWTA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "_PICK_BATCH_SIZE = 5000\n",
        "_CLASSIFICATION_SIZE = 7\n",
        "\n",
        "def one_hot_encode(x,size=_CLASSIFICATION_SIZE):\n",
        "  label = {'angry':0,'disgust':1,'fear':2,'happy':3,'neutral':4,'sad':5,'surprise':6}\n",
        "  x = label[x]\n",
        "  ohe = np.zeros(size)\n",
        "  ohe[x]=1\n",
        "  return ohe\n",
        "\n",
        "training_images_labels_list_all = []\n",
        "for training_directory_label in _TRAINING_DIRECTORIES_LABELS:\n",
        "  label = training_directory_label[1]\n",
        "  directory = training_directory_label[0]\n",
        "  print(directory)\n",
        "  for filename in os.listdir(directory):\n",
        "      if filename.endswith('.jpg') or filename.endswith('.png') or filename.endswith('.jpeg'):\n",
        "        if(len(training_images_labels_list_all)%100==0):\n",
        "          print('.',end='')\n",
        "        image = cv2.imread(os.path.join(directory, filename))\n",
        "        training_images_labels_list_all.append([image,label])\n",
        "      else:\n",
        "          print(f'Unable to read image: {filename}')\n",
        "\n",
        "print(f'Total images in the training set: {len(training_images_labels_list_all)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghl2h7F1mqI5",
        "outputId": "3f1c39b8-2236-450a-c443-7c6c14884774"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/MyDrive/A2_Data/T3_Data/images/train/angry\n",
            "........................................drive/MyDrive/A2_Data/T3_Data/images/train/disgust\n",
            ".....drive/MyDrive/A2_Data/T3_Data/images/train/fear\n",
            ".........................................drive/MyDrive/A2_Data/T3_Data/images/train/happy\n",
            ".......................................................................drive/MyDrive/A2_Data/T3_Data/images/train/neutral\n",
            "..................................................drive/MyDrive/A2_Data/T3_Data/images/train/sad\n",
            "..................................................drive/MyDrive/A2_Data/T3_Data/images/train/surprise\n",
            "................................Total images in the training set: 28823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6EAKKJrXc9Ok"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test train split\n",
        "def get_col(array,index):\n",
        "  arr = []\n",
        "  for row in array:\n",
        "    arr.append(row[index])\n",
        "  return arr\n",
        "\n",
        "def encode_labels(y):\n",
        "  arr = []\n",
        "  for label in y:\n",
        "    arr.append(one_hot_encode(label))\n",
        "  return arr\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = get_col(training_images_labels_list_all,0)\n",
        "y = get_col(training_images_labels_list_all,1)\n",
        "y = encode_labels(y)\n",
        "X_train_imgs_locations, X_test_img_locations, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "_3EHfWEWrYla"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train_imgs_locations), len(y_train), len(X_test_img_locations), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC4rTptasvIf",
        "outputId": "0c49e986-e9c5-4b5f-be37-6eff359d4583"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23058, 23058, 5765, 5765)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# given a image_location, get that image\n",
        "def get_img_from_location(location):\n",
        "  if location.endswith('.jpg') or location.endswith('.png') or location.endswith('.jpeg'):\n",
        "            # Read the image using OpenCV\n",
        "            image = cv2.imread(location)\n",
        "            # Append the image to the list and its name too\n",
        "            #print(np.shape(image))\n",
        "            if image is None:\n",
        "                print(f'Unable to read image: {location}')\n",
        "  return image\n",
        "\n",
        "# given a list of images_locations, get those images\n",
        "def get_imgs_from_locations(locations):\n",
        "  images = []\n",
        "  for location in locations:\n",
        "    images.append(get_img_from_location(location))\n",
        "  return images"
      ],
      "metadata": {
        "id": "JM9K6Q1-36Ye"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#config model\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
        "  try:\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=14336)]) # Notice here\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Virtual devices must be set before GPUs have been initialized\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3VbwYOf7pgy",
        "outputId": "9ffdeb67-558f-4d18-a597-822690219ee1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#declare model\n",
        "history = []\n",
        "model = None\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "_IMAGES_SHAPE = (48, 48, 3)\n",
        "epochs = 20\n",
        "batch_size = 10\n",
        "model = Sequential([\n",
        "    Conv2D(2, (2, 2), activation='relu', input_shape=_IMAGES_SHAPE),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(4, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(8, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(16, (2, 2), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.6),\n",
        "    Dense(7, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Xy4GwwZnYvgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pick a number of images from the training set to train model on\n",
        "train_images_list = X_train_imgs_locations\n",
        "train_labels_list = y_train\n"
      ],
      "metadata": {
        "id": "QsPEqde1Y7Kx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_images_list, train_labels_list, test_size=0.2, random_state=42)\n",
        "np.shape(X_train), np.shape(y_train), np.shape(X_val), np.shape(y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fb4o_oqicdp",
        "outputId": "c7919501-6192-4937-a5ba-09dc0b2e85a2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((18446, 48, 48, 3), (18446, 7), (4612, 48, 48, 3), (4612, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    zoom_range=0.1\n",
        ")\n",
        "datagen.fit(X_train)\n",
        "_IMAGES_SHAPE = (48, 48, 3)\n",
        "# Model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=_IMAGES_SHAPE),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(_CLASSIFICATION_SIZE, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(np.shape(X_train), np.shape(y_train), np.shape(X_val), np.shape(y_val))\n",
        "# Train the model\n",
        "with tf.device('/device:GPU:0'):\n",
        "  history = model.fit(np.array(X_train), np.array(y_train),  batch_size=32, validation_data=(np.array(X_val), np.array(y_val)), epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-hR86Zb7kKz",
        "outputId": "8481ac9e-73db-48cb-e81b-9d6f4f80dc45"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18446, 48, 48, 3) (18446, 7) (4612, 48, 48, 3) (4612, 7)\n",
            "Epoch 1/50\n",
            "577/577 [==============================] - 6s 7ms/step - loss: 3.2488 - accuracy: 0.2670 - val_loss: 1.7166 - val_accuracy: 0.3077\n",
            "Epoch 2/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 1.6734 - accuracy: 0.3587 - val_loss: 1.8235 - val_accuracy: 0.2758\n",
            "Epoch 3/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 1.5475 - accuracy: 0.4065 - val_loss: 1.5515 - val_accuracy: 0.4187\n",
            "Epoch 4/50\n",
            "577/577 [==============================] - 4s 7ms/step - loss: 1.4772 - accuracy: 0.4481 - val_loss: 1.5900 - val_accuracy: 0.4016\n",
            "Epoch 5/50\n",
            "577/577 [==============================] - 4s 6ms/step - loss: 1.3897 - accuracy: 0.4763 - val_loss: 1.5477 - val_accuracy: 0.4254\n",
            "Epoch 6/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 1.3120 - accuracy: 0.5103 - val_loss: 1.6845 - val_accuracy: 0.4113\n",
            "Epoch 7/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 1.2406 - accuracy: 0.5416 - val_loss: 1.7608 - val_accuracy: 0.4395\n",
            "Epoch 8/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 1.1607 - accuracy: 0.5768 - val_loss: 1.8535 - val_accuracy: 0.4202\n",
            "Epoch 9/50\n",
            "577/577 [==============================] - 4s 6ms/step - loss: 1.0918 - accuracy: 0.5988 - val_loss: 1.7599 - val_accuracy: 0.3985\n",
            "Epoch 10/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 1.0394 - accuracy: 0.6251 - val_loss: 1.9231 - val_accuracy: 0.4239\n",
            "Epoch 11/50\n",
            "577/577 [==============================] - 4s 8ms/step - loss: 0.9721 - accuracy: 0.6518 - val_loss: 1.9232 - val_accuracy: 0.4324\n",
            "Epoch 12/50\n",
            "577/577 [==============================] - 4s 7ms/step - loss: 0.9071 - accuracy: 0.6822 - val_loss: 2.5763 - val_accuracy: 0.4341\n",
            "Epoch 13/50\n",
            "577/577 [==============================] - 4s 7ms/step - loss: 0.8390 - accuracy: 0.7043 - val_loss: 2.4793 - val_accuracy: 0.4284\n",
            "Epoch 14/50\n",
            "577/577 [==============================] - 4s 6ms/step - loss: 0.7835 - accuracy: 0.7314 - val_loss: 2.4711 - val_accuracy: 0.4373\n",
            "Epoch 15/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 0.7372 - accuracy: 0.7484 - val_loss: 3.4874 - val_accuracy: 0.4434\n",
            "Epoch 16/50\n",
            "577/577 [==============================] - 5s 9ms/step - loss: 0.6851 - accuracy: 0.7644 - val_loss: 4.3956 - val_accuracy: 0.4352\n",
            "Epoch 17/50\n",
            "577/577 [==============================] - 4s 6ms/step - loss: 0.6260 - accuracy: 0.7861 - val_loss: 3.4632 - val_accuracy: 0.4226\n",
            "Epoch 18/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.5819 - accuracy: 0.8047 - val_loss: 4.0337 - val_accuracy: 0.4332\n",
            "Epoch 19/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.5395 - accuracy: 0.8197 - val_loss: 3.6426 - val_accuracy: 0.3957\n",
            "Epoch 20/50\n",
            "577/577 [==============================] - 4s 7ms/step - loss: 0.5210 - accuracy: 0.8317 - val_loss: 4.2210 - val_accuracy: 0.4267\n",
            "Epoch 21/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 0.4835 - accuracy: 0.8406 - val_loss: 5.3729 - val_accuracy: 0.4480\n",
            "Epoch 22/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.4673 - accuracy: 0.8534 - val_loss: 5.8437 - val_accuracy: 0.4438\n",
            "Epoch 23/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.4408 - accuracy: 0.8621 - val_loss: 4.8161 - val_accuracy: 0.4178\n",
            "Epoch 24/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 0.4310 - accuracy: 0.8654 - val_loss: 6.1749 - val_accuracy: 0.4430\n",
            "Epoch 25/50\n",
            "577/577 [==============================] - 4s 7ms/step - loss: 0.4111 - accuracy: 0.8778 - val_loss: 5.7623 - val_accuracy: 0.4306\n",
            "Epoch 26/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 0.3814 - accuracy: 0.8807 - val_loss: 6.4684 - val_accuracy: 0.4291\n",
            "Epoch 27/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.3914 - accuracy: 0.8866 - val_loss: 6.4595 - val_accuracy: 0.4330\n",
            "Epoch 28/50\n",
            "577/577 [==============================] - 4s 6ms/step - loss: 0.3564 - accuracy: 0.8941 - val_loss: 5.7843 - val_accuracy: 0.4211\n",
            "Epoch 29/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 0.3526 - accuracy: 0.9010 - val_loss: 6.1715 - val_accuracy: 0.4237\n",
            "Epoch 30/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.3564 - accuracy: 0.9018 - val_loss: 8.2074 - val_accuracy: 0.4367\n",
            "Epoch 31/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 0.3441 - accuracy: 0.9062 - val_loss: 9.0486 - val_accuracy: 0.4399\n",
            "Epoch 32/50\n",
            "577/577 [==============================] - 4s 6ms/step - loss: 0.3285 - accuracy: 0.9097 - val_loss: 6.5368 - val_accuracy: 0.4154\n",
            "Epoch 33/50\n",
            "577/577 [==============================] - 4s 7ms/step - loss: 0.3078 - accuracy: 0.9158 - val_loss: 8.4293 - val_accuracy: 0.4360\n",
            "Epoch 34/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.3219 - accuracy: 0.9174 - val_loss: 8.5449 - val_accuracy: 0.4321\n",
            "Epoch 35/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.2948 - accuracy: 0.9223 - val_loss: 7.4259 - val_accuracy: 0.4209\n",
            "Epoch 36/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.3019 - accuracy: 0.9243 - val_loss: 9.8449 - val_accuracy: 0.4111\n",
            "Epoch 37/50\n",
            "577/577 [==============================] - 4s 7ms/step - loss: 0.2707 - accuracy: 0.9310 - val_loss: 8.7588 - val_accuracy: 0.4291\n",
            "Epoch 38/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.2707 - accuracy: 0.9315 - val_loss: 10.2723 - val_accuracy: 0.4324\n",
            "Epoch 39/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.2550 - accuracy: 0.9371 - val_loss: 11.0239 - val_accuracy: 0.4397\n",
            "Epoch 40/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 0.2533 - accuracy: 0.9411 - val_loss: 12.6187 - val_accuracy: 0.4380\n",
            "Epoch 41/50\n",
            "577/577 [==============================] - 4s 7ms/step - loss: 0.2642 - accuracy: 0.9359 - val_loss: 11.0414 - val_accuracy: 0.4278\n",
            "Epoch 42/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.2495 - accuracy: 0.9449 - val_loss: 15.8764 - val_accuracy: 0.4310\n",
            "Epoch 43/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 0.2736 - accuracy: 0.9419 - val_loss: 10.8950 - val_accuracy: 0.4354\n",
            "Epoch 44/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.2617 - accuracy: 0.9385 - val_loss: 12.7314 - val_accuracy: 0.4406\n",
            "Epoch 45/50\n",
            "577/577 [==============================] - 4s 6ms/step - loss: 0.2382 - accuracy: 0.9480 - val_loss: 11.5774 - val_accuracy: 0.4328\n",
            "Epoch 46/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 0.2337 - accuracy: 0.9471 - val_loss: 13.4968 - val_accuracy: 0.4421\n",
            "Epoch 47/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.2538 - accuracy: 0.9474 - val_loss: 14.6144 - val_accuracy: 0.4508\n",
            "Epoch 48/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 0.2139 - accuracy: 0.9510 - val_loss: 13.8554 - val_accuracy: 0.4293\n",
            "Epoch 49/50\n",
            "577/577 [==============================] - 3s 5ms/step - loss: 0.2378 - accuracy: 0.9510 - val_loss: 18.2185 - val_accuracy: 0.4404\n",
            "Epoch 50/50\n",
            "577/577 [==============================] - 3s 6ms/step - loss: 0.2220 - accuracy: 0.9535 - val_loss: 13.4276 - val_accuracy: 0.4269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "# Define the file path where you want to save the model\n",
        "model_path = 'facial_expression_recognission_1.0.h5'\n",
        "\n",
        "# Save the model to the specified file path\n",
        "model.save(model_path)\n",
        "\n",
        "print(f\"Model saved to {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpAeh6Rm9yUf",
        "outputId": "b3e51b57-5c01-4905-ad88-2553eff5bfb2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to facial_expression_recognission_1.0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTlo7EpAnSFC",
        "outputId": "46fa0161-ccd0-4e05-c9d1-cebe9ff4172e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': [3.2488481998443604, 1.6733691692352295, 1.5474791526794434, 1.4772471189498901, 1.3896604776382446, 1.3119548559188843, 1.2405977249145508, 1.1606547832489014, 1.0917844772338867, 1.0394080877304077, 0.9721312522888184, 0.9070764183998108, 0.8389825224876404, 0.7834636569023132, 0.7372317910194397, 0.6850873231887817, 0.6260488629341125, 0.5818684697151184, 0.5394902229309082, 0.5209733843803406, 0.48346370458602905, 0.4673054814338684, 0.4407954514026642, 0.4309662878513336, 0.411111444234848, 0.3814040422439575, 0.39142948389053345, 0.35635247826576233, 0.3525505065917969, 0.35639098286628723, 0.344145804643631, 0.3285374045372009, 0.30775266885757446, 0.3219364583492279, 0.29480767250061035, 0.30188295245170593, 0.27067336440086365, 0.27073004841804504, 0.2549847662448883, 0.25330519676208496, 0.26421093940734863, 0.2494516372680664, 0.2735973000526428, 0.2616594135761261, 0.2382260113954544, 0.2336646020412445, 0.2538423240184784, 0.21386541426181793, 0.23775829374790192, 0.22200451791286469], 'accuracy': [0.26704975962638855, 0.3586685359477997, 0.40648379921913147, 0.4480646252632141, 0.47625502943992615, 0.51030033826828, 0.5416350364685059, 0.5768188238143921, 0.598774790763855, 0.6251219511032104, 0.65179443359375, 0.6821532845497131, 0.7043261528015137, 0.7314322590827942, 0.7484007477760315, 0.7643933892250061, 0.7861325144767761, 0.8046730756759644, 0.819689929485321, 0.8317250609397888, 0.8406158685684204, 0.8533557653427124, 0.862083911895752, 0.8653908967971802, 0.8777512907981873, 0.8807329535484314, 0.8866420984268188, 0.8940691947937012, 0.9010083675384521, 0.9018215537071228, 0.9061585068702698, 0.9097365140914917, 0.9157540798187256, 0.9174346923828125, 0.9222595691680908, 0.924265444278717, 0.931041955947876, 0.9314756393432617, 0.9371137619018555, 0.9410712122917175, 0.9358668327331543, 0.9448661208152771, 0.9418843984603882, 0.9385232329368591, 0.9479562044143677, 0.947143018245697, 0.9473598599433899, 0.9509921073913574, 0.9509921073913574, 0.9535400867462158], 'val_loss': [1.7165522575378418, 1.8235392570495605, 1.5514601469039917, 1.5900107622146606, 1.547682523727417, 1.6844724416732788, 1.760820746421814, 1.8534640073776245, 1.7598527669906616, 1.9230802059173584, 1.923193097114563, 2.57633376121521, 2.479325771331787, 2.471104145050049, 3.487351894378662, 4.395617961883545, 3.4632315635681152, 4.033658504486084, 3.6425535678863525, 4.2210211753845215, 5.372902870178223, 5.843677997589111, 4.81606388092041, 6.174872398376465, 5.762323379516602, 6.468394756317139, 6.45950174331665, 5.784265995025635, 6.171541690826416, 8.207428932189941, 9.048616409301758, 6.536836624145508, 8.429274559020996, 8.54488754272461, 7.42586612701416, 9.844919204711914, 8.75876235961914, 10.2723388671875, 11.023886680603027, 12.618705749511719, 11.04140853881836, 15.876405715942383, 10.894957542419434, 12.73137378692627, 11.577385902404785, 13.496811866760254, 14.614433288574219, 13.855364799499512, 18.2185001373291, 13.427593231201172], 'val_accuracy': [0.30767562985420227, 0.27580225467681885, 0.41869038343429565, 0.4015611410140991, 0.42541196942329407, 0.411318302154541, 0.4395056366920471, 0.4202081561088562, 0.398525595664978, 0.4238941967487335, 0.43235039710998535, 0.4340849816799164, 0.42844751477241516, 0.43733736872673035, 0.4434084892272949, 0.43516913056373596, 0.42259323596954346, 0.43321770429611206, 0.3957068622112274, 0.42671293020248413, 0.44796183705329895, 0.4438421428203583, 0.41782307624816895, 0.44297483563423157, 0.43061578273773193, 0.4290980100631714, 0.4330008625984192, 0.4210754632949829, 0.42367735505104065, 0.4366869032382965, 0.4399392902851105, 0.4154379963874817, 0.43603643774986267, 0.4321335554122925, 0.42085862159729004, 0.41110146045684814, 0.4290980100631714, 0.43235039710998535, 0.4397224485874176, 0.4379878640174866, 0.4277970492839813, 0.4310494363307953, 0.43538594245910645, 0.4405897557735443, 0.4327840507030487, 0.44210755825042725, 0.45078057050704956, 0.42931482195854187, 0.44037294387817383, 0.4269297420978546]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BldTAH1AonDB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}